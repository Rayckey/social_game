# -*- coding: utf-8 -*-
"""Copy of antisocial_game.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HQyRwJ47OEWTNFzRVsG7p2br9k-jHc40

<a href="https://arxiv.org/abs/1705.02445">
Three Changes to baseline Deep RNNs for Human Motion Prediction
</a><br>

<a href="https://arxiv.org/pdf/1910.01843.pdf">
Prediction of Human Full-Body Movements with
Motion Optimization and Recurrent Neural Networks
</a><br>

<a href="https://arxiv.org/pdf/2007.03672.pdf">
Long-term Human Motion Prediction with Scene Context
</a><br>

<a href="https://arxiv.org/pdf/2010.11061.pdf">
MADER</a><br>

<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Putting_Humans_in_a_Scene_Learning_Affordance_in_3D_Indoor_CVPR_2019_paper.pdf?fbclid=IwAR0bpVih5N5f_S_Ia38OUAnZhlFpkWa-FVOj9o0pi1yXTi8MKALi79NeW4I">
Putting Humans in a Scene: Learning Affordance in 3D Indoor Environments
</a><br>

<a href=https://papers.nips.cc/paper/2017/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf>
RNN for Predictive Learning 
</a><br>

Might be helpful for Class Problem:<br>
<a href="https://arxiv.org/pdf/1308.0850.pdf">
Generating Sequences with RNNs
</a>

<a href="https://crl.ucsd.edu/~elman/Papers/cs-counter.pdf">
A Recurrent Neural Network that Learns to Count
</a>

<a href="https://arxiv.org/pdf/1912.09260.pdf">
Deep Reinforcement Learning for Motion Planning of Mobile Robots
</a><br>

<a href="https://www.hindawi.com/journals/jr/2018/5781591/">
Dynamic Path Planning of Unknown Environment Based on Deep Reinforcement Learning
</a>
"""

# get git lmao
!git clone https://github.com/Rayckey/social_game.git

# Commented out IPython magic to ensure Python compatibility.
# %cd social_game
# read a scene
import json
scene_name = '1'
f = open('saved_scene_tasks_order/' + scene_name +'.json')
scene_dict = json.load(f)

import numpy as np
import random as rand
from termcolor import colored
import matplotlib.pyplot as plt
from pylab import rcParams
import matplotlib
from matplotlib import patches
from scipy.spatial import ConvexHull

!pip install colorama
from colorama import Fore, Back, Style 



# from stack overflow
def line_intersection(line1, line2):
    xdiff = (line1[0][0] - line1[1][0], line2[0][0] - line2[1][0])
    ydiff = (line1[0][1] - line1[1][1], line2[0][1] - line2[1][1]) 
    
    def det(a, b):
        return a[0] * b[1] - a[1] * b[0]

    div = det(xdiff, ydiff)
    if div == 0:
       raise Exception('lines do not intersect')

    d = (det(*line1), det(*line2))
    x = det(d, xdiff) / div
    y = det(d, ydiff) / div
    return x, y

# from me 


SPRITE_IMAGE_SIZE = 128
SPRITE_SCALING = 0.25
SPRITE_SIZE = SPRITE_IMAGE_SIZE * SPRITE_SCALING


COLLISION_THRESHOLD = 3
GOAL_THRESHOLD = 75

# SPRITE_SCALING = 0.5

SCREEN_WIDTH = 600
SCREEN_HEIGHT = 600
SCREEN_TITLE = "Move Sprite Social"

MOVEMENT_SPEED = 2
ANGLE_SPEED = 2

class Actor():
    """ Actor class almost straight from the arcade"""

    def __init__(self, histories):
        """ Set up the Actor """

        # Create a variable to hold our speed and angle
        self.speed = 0
        self.angle = 0

        # get ready to record the actions
        self.histories = histories
        self.center_x = histories['init_pos'][0]
        self.center_y = histories['init_pos'][1]
        self.rad_angle_hist = []
        self.speed_hist = []
        self.states = [[self.center_x,self.center_y, 0]]

    def processStates(self):
        for idx in range(len(self.histories["d_angle"])):
          self.change_angle = self.histories["d_angle"][idx]
          self.speed = self.histories["speed"][idx]
          self.update()
          self.states.append( [self.center_x, self.center_y, 
                              np.radians(self.angle)] )
          self.speed_hist.append(self.speed)
          self.rad_angle_hist.append(np.radians(self.angle))

    def update(self):
        # Convert angle in degrees to radians.
        angle_rad = np.radians(self.angle)

        # Rotate the ship
        self.angle += self.change_angle

        # Use math to find our change based on our speed and angle
        self.center_x += -self.speed * np.sin(angle_rad)
        self.center_y += self.speed * np.cos(angle_rad)



class roomEnv:
  # this is the main environment
  #TODO 
  # get AI gym working here
  def __init__(self, task=None):
    self.state = np.array([0.5, 0.5, 0]) # state 3 starts on x axis, rotates counter clock wise

    # set up game stuff
    self.actors = None
    self.obstacles = None
    self.interests = None

    # dynamics for the robot, it's holonomic 
    self.A = np.identity(3)
    self.B = np.diag(np.random.rand(3))
    self.range = 1.

    # Creating a random task
    # tasks = ['blue', 'green', 'red', 'socialize', 'isolate']
    # tasks = ['blue', 'green', 'red']
    self.task = task


  def loadActors(self,scene_dict):
    self.actors = scene_dict

    for actor_name in self.actors.keys():
      actor = Actor(self.actors[actor_name])
      actor.processStates()
      self.actors[actor_name]['hist'] = np.array(actor.states)


  def getMeasurement(self):
    # just in case we have sensors, I don't think we do but you know 
    # 1D lidar that just scans ahead
    lidar_end = (self.state[0] + np.cos(self.state[2])*self.range , self.state[1] + np.sin(self.state[2])**self.range)
    lidar_line = ( (self.state[0],self.state[1]) , lidar_end)

    collisions = []
    min_dist = 1.
    for obs in self.obstacles:
      # print(lidar_end)
      # print(obs)
      try:
        collision = line_intersection(lidar_line, obs)
      except:
        continue
      dist = np.sqrt( collision[0]**2. + collision[1]**2. ) 
      if dist < min_dist:
        min_dist = dist

    return min_dist


  def propagateDynamics(self, input_vec):
    # strictly for the holomoic robot
    # actors follow the rules on arcade game
    self.state =  self.A @ self.state + self.B @ input_vec
    if self.state[2] > 2.*np.pi:
      self.state[2] -= 2.*np.pi
    elif self.state[2] < -2.*np.pi:
      self.state[2] += 2.*np.pi

  def genRoom1(self):
    self.obstacles = []
    self.interests = []
    # Draw obstacles as rectancles
    line1 = ( (SCREEN_WIDTH / 3 , 0), 
              (SCREEN_WIDTH / 3 , SCREEN_WIDTH / 3) )
    
    line2 = ( (2 * SCREEN_WIDTH / 3, 2 * SCREEN_HEIGHT / 3), 
              (SCREEN_WIDTH, 2 * SCREEN_HEIGHT / 3) )
    
    line3 = ( (0 , 2 * SCREEN_HEIGHT / 3), 
              (SCREEN_WIDTH / 2, 2 * SCREEN_HEIGHT / 3))

    self.obstacles.append(self.genObstacle(line1,SPRITE_SIZE))
    self.obstacles.append(self.genObstacle(line2,SPRITE_SIZE))
    self.obstacles.append(self.genObstacle(line3,SPRITE_SIZE))

    # hmm interesting (points)
    self.interests.append((SCREEN_WIDTH / 6, SCREEN_HEIGHT / 6))
    self.interests.append((5 * SCREEN_WIDTH / 6, SCREEN_HEIGHT / 2))
    self.interests.append((SCREEN_WIDTH / 4, 5 * SCREEN_HEIGHT / 6))


  def genObstacle(self, line, sprite_size):
    # Draw obstacle as a rectancle
    sprite_half = sprite_size/2
    if line[0][0] < line[1][0] or line[0][1] < line[1][1]:
      #pt1 on left of pt2
      upper_left =  self.genRec(line[0], sprite_size)[0]
      lower_right = self.genRec(line[1], sprite_size)[1]
      rec = (upper_left, lower_right)
      return rec
    else:
      # pt1 on right of pt2
      upper_left =  self.genRec(line[1], sprite_size)[0]
      lower_right = self.genRec(line[0], sprite_size)[1]
      rec = (upper_left, lower_right)
      return rec
    
  def genRec(self, point, sprite_size):
    sprite_half = sprite_size/2
    # gives upper left and lower right point
    rec = ( (point[0]-sprite_half , point[1]-sprite_half), 
           (point[0]+sprite_half , point[1]+sprite_half) )
    return rec

  def isValid(self, state, time_step = 0):
    # check with obstacles
    for obs in self.obstacles:
      if obs[0][0] < state[0] < obs[1][0] and obs[0][1] < state[1] < obs[1][1]:
        # print("Collision with Obstacles")
        return False
    
    for actor_name in self.actors.keys():
      actor = self.actors[actor_name]  
      check_coord = None

      if time_step > len(actor['hist']):
        check_coord = actor['hist'][-1]
      else:
        try:
          check_coord = actor['hist'][time_step]
        except:
          check_coord = actor['hist'][-1]

      dis_2 = np.sqrt((state[0] - check_coord[0])**2 + (state[1] - check_coord[1])**2)
      if dis_2 < COLLISION_THRESHOLD:
        # print("Collision with Agents")
        return False
    return True       

  # To check if the actor has reached the goal
  def isGoal(self, state):
    # 0 -> blue
    # 1 -> green
    # 2 -> red
    
    if (self.task == 'blue'):
      check_coord = self.interests[0]
    elif (self.task == 'green'):
      check_coord = self.interests[1]
    else:
      check_coord = self.interests[2]
      
    dis_2 = np.sqrt((state[0] - check_coord[0])**2 + (state[1] - check_coord[1])**2)
    if dis_2 < GOAL_THRESHOLD:
      return True
    return False
  
  def isBounds(self, state):
    if (state[0] < 0 or state[0] > 600):
      return False
    if (state[1] < 0 or state[1] > 600):
      return False
    return True
  
  def oneTimeReward(self, state):
    dis_2 = np.sqrt((state[0] - 350)**2 + (state[1] - 400)**2)
    if dis_2 < 100:
      return True
    return False

  def getGoalDistance(self,state):
    if (self.task == 'blue'):
      check_coord = self.interests[0]
    elif (self.task == 'green'):
      check_coord = self.interests[1]
    else:
      check_coord = self.interests[2]

    return np.sqrt((state[0] - check_coord[0])**2 + (state[1] - check_coord[1])**2)

  def getGoalCoordinates(self):
    if (self.task == 'blue'):
      check_coord = self.interests[0]
    elif (self.task == 'green'):
      check_coord = self.interests[1]
    else:
      check_coord = self.interests[2]
    return check_coord    
  
  def drawRoom(self):
    
    fig = plt.figure() 
    ax = fig.add_subplot(111) 
    for obs in self.obstacles:
      # Plot the obstacles:
      ax.add_patch(matplotlib.patches.Rectangle(obs[0], obs[1][0] - obs[0][0], 
                              obs[1][1] - obs[0][1] , color ='black' ) )

    # plot the interesting drinks
    for inte in self.interests:
      plt.plot(inte[0], inte[1], 'bo')

    plt.xlim(0, SCREEN_WIDTH)
    plt.ylim(0, SCREEN_HEIGHT)


  def drawActors(self):
      for actor_name in self.actors.keys():
        # print(self.actors[actor_name]['hist'][:,0])
        plt.plot( self.actors[actor_name]['hist'][:,0] , self.actors[actor_name]['hist'][:,1]) 
      # pass

  def drawNewActor(self, new_actor):
    plt.plot(new_actor[:,0], new_actor[:,1])

  def showActorsProps(self):
        for actor_name in self.actors.keys():
          for key_name in self.actors[actor_name].keys():
            print(key_name)
          break

# environment test
room = roomEnv()
room.genRoom1()

# History test

room = roomEnv()
room.genRoom1()
room.loadActors(scene_dict)

# plot test
room.drawRoom()
room.drawActors()
plt.show()

# valid test
room.isValid([5.07744938e+02, 2.67820250e+02, 3.83972435e-01], 500)

room.showActorsProps()

room.actors['roc']['hist'][500]

# Packages for Deep Q Learning
# !pip install h5py
# !pip install gym
# !pip install keras-rl

import numpy as np
import gym
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from keras.optimizers import Adam
from collections import deque
import os
import random

random.seed()
class Agent():
  def __init__(self, state_size, action_size, task):
    self.weight_backup      = "path_planning_TEST_" + task + "13.h5"
    self.state_size         = state_size
    self.action_size        = action_size
    self.memory             = deque(maxlen=10000)
    self.learning_rate      = 0.001
    self.gamma              = 0.9
    self.exploration_rate   = 1.0
    self.exploration_min    = 0.01
    self.exploration_decay  = 0.999
    self.brain              = self._build_model()
  def _build_model(self):
    # Neural Net for Deep-Q learning Model
    model = Sequential()
    model.add(Dense(600, input_dim=self.state_size, activation='relu'))
    model.add(Dense(600, activation='relu'))
    model.add(Dense(800, activation='relu'))
    model.add(Dense(self.action_size))
    model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))
    if os.path.isfile(self.weight_backup):
        model.load_weights(self.weight_backup)
        self.exploration_rate = self.exploration_min
    return model
  def save_model(self):
    self.brain.save(self.weight_backup)
  def act(self, state):
    if np.random.rand() <= self.exploration_rate:
        return random.randrange(self.action_size)
    act_values = self.brain.predict(state)
    # print(act_values)
    # print(np.argmax(act_values[0]))
    return np.argmax(act_values[0])
  def remember(self, state, action, reward, next_state, done):
    self.memory.append((state, action, reward, next_state, done))
  def replay(self, sample_batch_size):
    if len(self.memory) < sample_batch_size:
        return
    sample_batch = random.sample(self.memory, sample_batch_size)
    for state, action, reward, next_state, done in sample_batch:
        target = reward
        if not done:
          target = reward + self.gamma * np.amax(self.brain.predict(next_state)[0])
        target_f = self.brain.predict(state)
        target_f[0][action] = target
        self.brain.fit(state, target_f, epochs=5, verbose=0)
    if self.exploration_rate > self.exploration_min:
        self.exploration_rate *= self.exploration_decay

class Pathing:
  def __init__(self, room, begin_point, task, number_episodes=1000):
    self.sample_batch_size = 10
    self.episodes          = number_episodes
    self.env               = PathEnv(room, begin_point)
    self.state_size        = self.env.observation_space.shape[0]
    self.action_size       = self.env.action_space.n
    self.agent             = Agent(self.state_size, self.action_size, task)
  def run(self):
    try:
        for index_episode in range(self.episodes):
          state = self.env.reset()
          state = np.reshape(state, [1, self.state_size])
          done = False
          index = 0
          while not done:
            action = self.agent.act(state)
            next_state, reward, done, info = self.env.step(action)
            next_state = np.reshape(next_state, [1, self.state_size])
            # print(next_state)
            self.agent.remember(state, action, reward, next_state, done)
            state = next_state
            index += 1
          print("Episode {}# Steps: {}".format(index_episode, index))
          self.agent.replay(self.sample_batch_size)
    finally:
        self.agent.save_model()

import gym
from gym import error, spaces
from gym import utils
from gym.utils import seeding
import os, subprocess, time, signal
import math

TIME_STEP_MAX = 50000

class PathEnv(gym.Env):
  metadata = {'render.modes': ['human']}

  def __init__(self, room, begin_point):
    self.action_space = spaces.Discrete(4)
    self.observation_space = spaces.Box(low=np.array([0,0,0]), high=np.array([SCREEN_WIDTH, SCREEN_HEIGHT, TIME_STEP_MAX]), dtype=int)
    self.room = room
    self.begin_point = begin_point
    self.reset()
      
  def step(self, action):
    self._update_action(action)
    self.agent_history.append(np.array(self.state[:2].copy()))
    reward, done = self._check_reward_status()
    # print(self.state)
    self.total_reward += reward
    if (done):
      print(self.total_reward)
    return self.state, reward, done, {}

  def _update_action(self, action):
    # Moves:
    # 0 - up
    # 1 - right
    # 2 - down
    # 3 - left
    row, col, timestep = self.state

    if (room.isValid([row, col])):
      # Add current set to the visited state
      self.visited.add((row, col))

    valid_actions = self.action_valid()

    # If no valid actions
    if not valid_actions:
      mode = 'blocked'
    elif (action in valid_actions):
      mode = 'valid'
      if (action == 0):
        self.state[1] += 1
      elif (action == 1):
        self.state[0] += 1
      elif (action == 2):
        self.state[1] += -1
      else:
        self.state[0] += -1
    else:
      mode = 'invalid'

    self.state[2] = timestep+1
    self.mode = mode

  def _check_reward_status(self):
    row, col, timestep = self.state

    done = False
    if (self.mode == 'blocked'):
      reward = -0.4
    if (self.mode == 'valid'):
      reward = -0.04 - 0.001*(room.getGoalDistance(self.state[:2]))
    if (self.mode == 'invalid'):
      reward = -0.25
    if (row, col) in self.visited:
      reward = -0.4
    if (self.state[2] >= TIME_STEP_MAX):
      self.room.drawRoom()
      self.room.drawNewActor(np.array(self.agent_history))
      plt.show()
      plt.clf()
      done = True    
    if (self.room.isGoal(self.state[:2])):
      print(Fore.GREEN + "Goal Reached")
      self.room.drawRoom()
      self.room.drawNewActor(np.array(self.agent_history))
      plt.show()
      plt.clf()
      reward = TIME_STEP_MAX
      done = True
    return reward, done
  
  def action_valid(self):
    # 0 - up
    # 1 - right
    # 2 - down
    # 3 - left
    actions = [0, 1, 2, 3]

    row = self.state[0]
    col = self.state[1]
    timestep = self.state[2]

    # Check bounds of the room
    if (row == 0):
      actions.remove(3)
    elif (row == 600):
      actions.remove(1)
    
    if (col == 0):
      actions.remove(2)
    elif (col == 600):
      actions.remove(0)

    # Check to see if the space is filled by another agent
    if (row > 0 and not room.isValid([row-1, col], timestep)):
      actions.remove(3)
    if (row < 601) and not room.isValid([row+1, col], timestep):
      actions.remove(1)
    if (col > 0 and not room.isValid([row, col-1], timestep)):
      actions.remove(2)
    if (col < 601) and not room.isValid([row, col+1], timestep):
      actions.remove(0)

    #print(actions)
    return actions

  def reset(self):
    # Set the game back to its original state
    self.state = np.array(self.begin_point + [0])
    self.agent_history = []
    self.visited = set()
    self.total_reward = 0
    self.mode = 'start'
    self.mid = False
    return self.state

# ['blue', 'green', 'red']
task = 'red'
room = roomEnv(task)
room.genRoom1()
room.loadActors(scene_dict)
goal = room.getGoalCoordinates()

"""
count = 1
interval = 250
print("Goal: ", goal)
while (count < 4):
  for _ in range(3):
    newMax = interval*count
    while (True):
      point1 = round(goal[0] + newMax*math.cos(random.uniform(-math.pi, math.pi)))
      point2 = round(goal[1] + newMax*math.sin(random.uniform(-math.pi, math.pi)))

      newSet = [point1, point2]

      if (room.isValid(newSet)):
        if (not room.isGoal(newSet)):
          if (room.isBounds(newSet)):
            break

    print("\nStart point: ", newSet)
    path_learning = Pathing(room, [400,100], task)
    path_learning.run()

  count += 1
"""
for _ in range(3):
  path_learning = Pathing(room, [100,150], task)
  path_learning.run()



